# Story 3.1: LiteLLM Integration

> **Epic:** 3 - Multi-Provider AI Engine
> **Sprint:** S2 (Week 2)
> **Priority:** P0 (AI engine needed by Agent Router)
> **Points:** 5
> **Type:** Backend/AI
> **Agent:** @dev
> **Depends on:** Story 1.4 (Settings — provider config)

---

## Status

**Draft**

---

## Story

**As a** developer,
**I want** a unified AI provider abstraction using LiteLLM,
**so that** agents can use any supported AI provider without code changes.

---

## Acceptance Criteria

1. `ai_engine.py` module wraps LiteLLM for provider-agnostic completions
2. Supports: `anthropic/claude-sonnet-4-5-20250929`, `openai/gpt-4o`, `google/gemini-2.0-flash`
3. Streaming responses supported for all three providers
4. Provider selection from user settings (changeable per-session)
5. Automatic retry with exponential backoff on transient errors
6. Token usage tracking per session (displayed in UI)
7. Graceful error messages when API key is invalid or quota exceeded

---

## Integration Verification

- **IV1:** Same agent prompt produces functionally equivalent results across all 3 providers
- **IV2:** Streaming works identically for all providers (same message format)
- **IV3:** Existing agent definitions (.claude/agents/*.md) are used as system prompts without modification

---

## CodeRabbit Integration

> **CodeRabbit Integration**: Enabled
> **Profile**: assertive
> **Quality Gates**: block_on_critical=true, require_resolution=true

### Applicable Path Instructions

**`scripts/**/*.py`** (ai_engine.py):
- CRITICAL: API keys must NEVER appear in logs, error messages, or exception traces
- CRITICAL: LiteLLM calls must pass `api_key` parameter (not env var) — BYOK model
- HIGH: Retry logic must use exponential backoff (not fixed delay)
- HIGH: Custom exceptions (AIAuthError, AIRateLimitError) must not expose key in message
- HIGH: Streaming response must handle partial chunks and connection drops
- MEDIUM: Token tracking must not leak between sessions

**`tests/**/*.py`** (test_ai_engine.py):
- VALIDATE: All LiteLLM calls properly mocked (never call real AI providers)
- VALIDATE: Retry behavior tested with transient errors
- VALIDATE: Token usage tracking accuracy
- DO NOT: Require docstrings in test functions

### Review Focus for This Story
- `_get_model_string()` must handle unknown provider/model combos gracefully (not crash)
- Streaming async generator must properly close on cancellation
- `update_settings()` must not leave stale API keys in memory after provider switch
- Error messages must distinguish auth errors from rate limits from connection errors

---

## Tasks / Subtasks

- [ ] **T1: Create `scripts/ai_engine.py` — AIEngine class** (AC: 1, 2, 4)
  - [ ] T1.1: Define `AIEngine` class with `__init__(self, settings: Settings)`
  - [ ] T1.2: Store provider, model, api_key from settings
  - [ ] T1.3: Implement `_get_model_string() -> str` mapping (provider, model) to LiteLLM format
  - [ ] T1.4: Model map: `("anthropic", "claude-sonnet") → "anthropic/claude-sonnet-4-5-20250929"`, `("openai", "gpt-4o") → "openai/gpt-4o"`, `("google", "gemini-pro") → "gemini/gemini-2.0-flash"`, `("ollama", "llama3") → "ollama/llama3.2"`
  - [ ] T1.5: Implement `update_settings(settings: Settings)` for per-session provider switching

- [ ] **T2: Implement streaming chat completion** (AC: 2, 3)
  - [ ] T2.1: Implement `async chat_completion(messages, tools, stream, temperature) -> AsyncGenerator`
  - [ ] T2.2: Call `litellm.acompletion()` with model string, messages, tools, stream=True
  - [ ] T2.3: Pass `api_key` parameter to LiteLLM (not env var — BYOK model)
  - [ ] T2.4: Yield chunks from streaming response
  - [ ] T2.5: Handle both text chunks and tool_call chunks in the stream

- [ ] **T3: Implement classify method for Agent Router** (AC: 1)
  - [ ] T3.1: Implement `async classify(message, agents) -> ClassificationResult`
  - [ ] T3.2: Build classification tool definition (route_to_agent function)
  - [ ] T3.3: Send to LLM with `tool_choice="required"` to force classification
  - [ ] T3.4: Parse tool_calls response → ClassificationResult

- [ ] **T4: Implement retry and error handling** (AC: 5, 7)
  - [ ] T4.1: Wrap LiteLLM calls with retry decorator: 3 retries, exponential backoff (1s, 2s, 4s)
  - [ ] T4.2: Catch `litellm.AuthenticationError` → raise AIAuthError("Invalid API key for {provider}")
  - [ ] T4.3: Catch `litellm.RateLimitError` → raise AIRateLimitError("Rate limit exceeded")
  - [ ] T4.4: Catch `litellm.APIConnectionError` → raise AIConnectionError("Cannot reach {provider}")
  - [ ] T4.5: Define custom exceptions: `AIAuthError`, `AIRateLimitError`, `AIConnectionError`

- [ ] **T5: Implement token tracking** (AC: 6)
  - [ ] T5.1: Track `prompt_tokens`, `completion_tokens`, `total_tokens` per call
  - [ ] T5.2: Accumulate per session in `SessionTokenUsage` dataclass
  - [ ] T5.3: Method `get_session_usage(session_id) -> SessionTokenUsage`
  - [ ] T5.4: Extract usage from LiteLLM response metadata

- [ ] **T6: Add dependencies** (AC: 1)
  - [ ] T6.1: Add `litellm>=1.50.0` to requirements.txt

- [ ] **T7: Write unit tests**
  - [ ] T7.1: `tests/test_ai_engine.py` — Test AIEngine initialization with settings
  - [ ] T7.2: Test `_get_model_string()` for all provider/model combinations
  - [ ] T7.3: Test streaming with mocked LiteLLM (mock `litellm.acompletion`)
  - [ ] T7.4: Test classify with mocked tool_calls response
  - [ ] T7.5: Test retry behavior on transient errors
  - [ ] T7.6: Test auth error handling (invalid key)
  - [ ] T7.7: Test token tracking accumulation
  - [ ] T7.8: Ensure all tests pass: `pytest tests/test_ai_engine.py -v`

---

## Dev Notes

### AI Engine Architecture

[Source: architecture.md#5.3 AI Engine]

```python
class AIEngine:
    def __init__(self, settings: Settings):
        self.provider = settings.ai_provider
        self.model = settings.ai_model
        self.api_key = settings.ai_api_key

    async def chat_completion(
        self,
        messages: list[dict],
        tools: Optional[list[dict]] = None,
        stream: bool = True,
        temperature: float = 0.1,
    ) -> AsyncGenerator[str, None]:
        response = await litellm.acompletion(
            model=self._get_model_string(),
            messages=messages,
            tools=tools,
            stream=stream,
            temperature=temperature,
            api_key=self.api_key,
        )
        async for chunk in response:
            yield chunk

    def _get_model_string(self) -> str:
        model_map = {
            ("anthropic", "claude-sonnet"): "anthropic/claude-sonnet-4-5-20250929",
            ("anthropic", "claude-haiku"): "anthropic/claude-haiku-4-5-20251001",
            ("openai", "gpt-4o"): "openai/gpt-4o",
            ("openai", "gpt-4o-mini"): "openai/gpt-4o-mini",
            ("google", "gemini-pro"): "gemini/gemini-2.0-flash",
            ("ollama", "llama3"): "ollama/llama3.2",
        }
        return model_map.get((self.provider, self.model), self.model)
```

### Provider Configuration

[Source: architecture.md#5.3 AI Engine - Provider Configuration]

| Provider | Auth | Streaming | Function Calling | Local |
|----------|------|-----------|------------------|-------|
| Anthropic (Claude) | API key | Yes | Yes (tools) | No |
| OpenAI (GPT) | API key | Yes | Yes (tools) | No |
| Google (Gemini) | API key | Yes | Yes (tools) | No |
| Ollama | None | Yes | Limited | Yes |

### LiteLLM Model String Format

LiteLLM uses `provider/model` format. The `api_key` is passed per-call (not via env var) because this is BYOK.

### File Locations

| File | Location | Purpose |
|------|----------|---------|
| AI Engine | `scripts/ai_engine.py` | LiteLLM wrapper |
| Unit tests | `tests/test_ai_engine.py` | Engine tests |

### Import Pattern

```python
from scripts.settings import Settings, SettingsManager
from scripts.ai_engine import AIEngine

manager = SettingsManager()
settings = manager.load()
engine = AIEngine(settings)
```

---

## Testing

- **Framework:** pytest
- **Location:** `tests/test_ai_engine.py`
- **Coverage target:** 80%
- **Mock strategy:** Mock `litellm.acompletion` — never call real AI providers in tests

```python
import pytest
from unittest.mock import AsyncMock, patch
from scripts.ai_engine import AIEngine
from scripts.settings import Settings

@pytest.fixture
def engine():
    settings = Settings(ai_provider="anthropic", ai_model="claude-sonnet", ai_api_key="sk-test")
    return AIEngine(settings)

def test_model_string_anthropic(engine):
    assert engine._get_model_string() == "anthropic/claude-sonnet-4-5-20250929"

def test_model_string_openai():
    s = Settings(ai_provider="openai", ai_model="gpt-4o", ai_api_key="sk-test")
    e = AIEngine(s)
    assert e._get_model_string() == "openai/gpt-4o"

@pytest.mark.asyncio
@patch("litellm.acompletion")
async def test_streaming(mock_completion, engine):
    mock_completion.return_value = AsyncMock()
    # Test streaming behavior...
```

Run tests: `pytest tests/test_ai_engine.py -v`

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2026-02-05 | 0.1.0 | Story created from PRD + Architecture | River (SM) |
| 2026-02-05 | 0.1.1 | CodeRabbit integration enabled — assertive profile, path instructions + review focus added | River (SM) |

---

## Dev Agent Record

### Agent Model Used
_(To be filled by dev agent)_

### Debug Log References
_(To be filled by dev agent)_

### Completion Notes List
_(To be filled by dev agent)_

### File List
_(To be filled by dev agent)_

---

## QA Results
_(To be filled by QA agent)_
